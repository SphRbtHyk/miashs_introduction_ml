{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a6738aa",
   "metadata": {},
   "source": [
    "# Classification trees\n",
    "This lab is a level up from previous sessions ! There is no code snippet for demonstration, as you should now be able to answer the different questions by reading the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8de1755",
   "metadata": {},
   "source": [
    "## Classification trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489e6e6b",
   "metadata": {},
   "source": [
    "### Building a tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e241776f",
   "metadata": {},
   "source": [
    "**Questions**:\n",
    "1. Build a decision tree on your previously processed dataset, using `DecisionTreeClassifier` (https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier).\n",
    "2. What are the different available hyperparameters in sklearn for decision trees ?\n",
    "3. Select optimum hyperparameters using a cross validation setting.\n",
    "4. Return optimum parameters combination using cross validation and final accuracy (or recall or precision depending on your wanted target)\n",
    "5. Explain how the attribute `feature_importances_` is computed.\n",
    "6. Plot an elbow plot the most important features in your dataset and select the subset of most important features.\n",
    "7. Run classification experiment and see if results are impacted.\n",
    "8. **Bonus**: Use the features selected by the trees and test the two other classification method we tested: KNN and Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33911cb0",
   "metadata": {},
   "source": [
    "### Representation\n",
    "\n",
    "Trees can be represented using the `tree.plot_tree` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843168b8",
   "metadata": {},
   "source": [
    "**Questions**:\n",
    "1. Plot the previous tree.\n",
    "2. Explain every single information provided by the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2ece54",
   "metadata": {},
   "source": [
    "## Random forests\n",
    "One of the major drawbacks of decision trees is their tendency to severely overfit on the data given as input. To improve this behavior, multiples trees are often built on randomly drawn subset of individuals and their contribution to the classification voted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b33fa5",
   "metadata": {},
   "source": [
    "**Questions**:\n",
    "1. Build a random forest using the class`RandomForestClassifier` https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.\n",
    "2. Identify the hyperparameters and select the optimum combination using cross validation.\n",
    "3. Train the random forest on the whole dataset and return the final score.\n",
    "4. **Bonus**: compare the resilience to over fitting of trees and random forests by performing the following experiments: split the dataset into train and test, compare the results of the trees on train and test with the results of the random forest on the same dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
