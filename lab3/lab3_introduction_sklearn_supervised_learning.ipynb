{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "949f0928",
   "metadata": {},
   "source": [
    "# Lab3: Introduction to supervised learning\n",
    "This lab will be separated into two parts:\n",
    "\n",
    "1. First, we will code ourselves a random-based classifier and evaluate it using k-fold validation on the Pokemon dataset.\n",
    "\n",
    "2. We will learn to do the same thing using the [sklearn](https://scikit-learn.org/stable/) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2656a54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef23e7c8",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "\n",
    "Load the Pokemon dataset (or the `pre_processed.csv` one we did in the previous session)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03d8ab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../pokemon.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94b0b44",
   "metadata": {},
   "source": [
    "Extract 2 features of your choice into an array `X` and a target array `y` (conventional notations of `sklearn`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32d3dc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am only extracting a few subset of variables, \n",
    "# because we are working on the random classifier, but you can take all the features we studied last lab\n",
    "X = df[['sp_attack', 'sp_defense']].values\n",
    "y = df[\"is_legendary\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c664950",
   "metadata": {},
   "source": [
    "## Coding our own solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586918b5",
   "metadata": {},
   "source": [
    "### Coding a random classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715a1c80",
   "metadata": {},
   "source": [
    "1. Implement the simplest possible classifier: given a numpy vector and its ground truth, return a random value between 0 and 1 (use `numpy.random.binomial`). Make $p$ (the probability of being classified as 1) a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "42fb8162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_classifier(X, p: float = .5):\n",
    "    \"\"\"Random classifier: given a numpy vector X, return either the value 0 or 1, with probability p.\n",
    "    \n",
    "    Example:\n",
    "        random_classifier(np.array([[1, 2, 3]])) returns 1\n",
    "    \"\"\"\n",
    "    return np.random.binomial(1, p, X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "44109938-6fcc-4269-8aba-539bc72b9454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 1, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_classifier(np.array([np.random.normal(1, 10, 10) for _ in range(10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca01dee",
   "metadata": {},
   "source": [
    "2. Apply this classifier on all values in the `X` numpy matrix and store it in `y_predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f0fd7945-09a4-4e53-b55e-588a72d0929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = random_classifier(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "67a68dd4-f086-499a-a8ac-83346e1ae701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1, 1, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First ten individuals predicted\n",
    "y_predict[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcc8366",
   "metadata": {},
   "source": [
    "3. Create the four evaluation functions we saw during lecture 4, that takes as iput :\n",
    "- `accuracy`\n",
    "- `recall`\n",
    "- `f1_score`\n",
    "- `precision`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "af885b01-f316-4fbc-b7f3-1479696bcc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    return np.sum(y_pred == y_true)/y_pred.shape[0]\n",
    "\n",
    "def recall(y_pred, y_true):\n",
    "    return np.sum((y_pred == 1) & (y_true==1))/(np.sum((y_pred == 1) & (y_true==1)) + np.sum((y_pred == 0) & (y_true==1)))\n",
    "\n",
    "def precision(y_pred, y_true):\n",
    "    return np.sum((y_pred == 1) & (y_true==1))/(np.sum((y_pred == 1) & (y_true==1)) + np.sum((y_pred == 1) & (y_true==0)))\n",
    "\n",
    "def f1_score(y_pred, y_true):\n",
    "    \"\"\"Compute the F1 score between y and y_predict.\n",
    "    \"\"\"\n",
    "    computed_precision = precision(y_pred, y_true)\n",
    "    computed_recall = recall(y_pred, y_true)\n",
    "    return 2 * (computed_precision*computed_recall)/(computed_precision + computed_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f10c6a",
   "metadata": {},
   "source": [
    "4. Apply these functions to `y` and `y_predict` and draw conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "95bde225-f5c7-4bbe-977e-8ef50dead2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Accuracy\n",
      "0.5143570536828964\n",
      "========= Precision\n",
      "0.5\n",
      "========= Recall\n",
      "0.08997429305912596\n",
      "========= F1 score\n",
      "0.15250544662309368\n"
     ]
    }
   ],
   "source": [
    "print(\"========= Accuracy\")\n",
    "print(f\"{accuracy(y, y_predict)}\")\n",
    "print(\"========= Precision\")\n",
    "print(f\"{precision(y, y_predict)}\")\n",
    "print(\"========= Recall\")\n",
    "print(f\"{recall(y, y_predict)}\")\n",
    "print(\"========= F1 score\")\n",
    "print(f\"{f1_score(y, y_predict)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033e8d4e",
   "metadata": {},
   "source": [
    "### Separation between tests and train\n",
    "We will evaluate our algorithm by \"training\" it on a subset of the data `X_train`, `y_train` and evaluate it on the data `X_test`, and compare `y_test` with the ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61544d31",
   "metadata": {},
   "source": [
    "1. Is there a training phase of the random classifier ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f2592b-2b5c-4520-b9db-f40f08a4b758",
   "metadata": {},
   "source": [
    "No !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb314bd7",
   "metadata": {},
   "source": [
    "2. Create a function `split_train_test` that takes as input a matrix `X` and a target `y` and randomly splits into two matrixes `X_train` and `X_test` and a target `y_train` and `y_test`. You can use the function `numpy.random.choice`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e1dae22d-d123-4b93-bef5-4692c26f0636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 40  79]\n",
      " [ 80 120]\n",
      " [ 85  50]\n",
      " ...\n",
      " [ 70  80]\n",
      " [ 97  80]\n",
      " [ 40  40]]\n"
     ]
    }
   ],
   "source": [
    "def split_train_test(X, y, p_train = 0.7):\n",
    "    # Select train index\n",
    "    train_indexes = np.random.choice(np.arange(X.shape[0]), replace=False, size=round(p_train*X.shape[0]))\n",
    "    # Get test indexes as a difference\n",
    "    test_indexes = np.array(list(set(np.arange(X.shape[0])) - set(train_indexes)))\n",
    "    # Index X and y accordingly\n",
    "    return X[train_indexes], X[test_indexes], y[train_indexes], y[test_indexes]\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_train_test(X, y)\n",
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41404f7d",
   "metadata": {},
   "source": [
    "3. Predict the value on the test dataset `X_test` on `y_test_predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a30916e4-5742-44ac-ab4c-80b02ad57778",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predict = random_classifier(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4421e887",
   "metadata": {},
   "source": [
    "4. Compute the accuracy, precision, recall, f1_score by comparing `y_test_predict` to `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c6cd7315-e491-47b5-b148-aa75a12aad2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Accuracy\n",
      "0.55\n",
      "========= Precision\n",
      "0.6666666666666666\n",
      "========= Recall\n",
      "0.13793103448275862\n",
      "========= F1 score\n",
      "0.2285714285714286\n"
     ]
    }
   ],
   "source": [
    "print(\"========= Accuracy\")\n",
    "print(f\"{accuracy(y_test, y_test_predict)}\")\n",
    "print(\"========= Precision\")\n",
    "print(f\"{precision(y_test, y_test_predict)}\")\n",
    "print(\"========= Recall\")\n",
    "print(f\"{recall(y_test, y_test_predict)}\")\n",
    "print(\"========= F1 score\")\n",
    "print(f\"{f1_score(y_test, y_test_predict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c8696da2-b054-4397-a179-3d4bbfa05a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7k/nf094zsx3c3fvbzcnlq9dn000000gn/T/ipykernel_9435/3266681177.py:1: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "  pd.value_counts(y_test)\n",
      "/var/folders/7k/nf094zsx3c3fvbzcnlq9dn000000gn/T/ipykernel_9435/3266681177.py:3: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "  pd.value_counts(y_test_predict)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    124\n",
       "1    116\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(y_test)\n",
    "\n",
    "pd.value_counts(y_test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5157c0fe",
   "metadata": {},
   "source": [
    "5. Can you see what is the limitation of using simply accuracy ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f5962f",
   "metadata": {},
   "source": [
    "Precision reflects the repartition of the data, in the case of an unbalanced dataset, if we predicted always the same value we would get a good score even though our classifier is a constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69246108",
   "metadata": {},
   "source": [
    "### K-fold validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4c6546",
   "metadata": {},
   "source": [
    "The other, more robust approach we saw in class is k fold validation, which consists in using *k-1* fold for training and 1 fold for testing. We then compute an average/median of the performance metrics over all experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142ca406",
   "metadata": {},
   "source": [
    "1. Create a function `k_fold_train_test` that will first shuffle an input matrix and then divide into k-fold with the number of folds specified as input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccd380a",
   "metadata": {},
   "source": [
    "2. Use the k-fold algorithm to compute the average accuracy and recall the k folds. The algorithm will:\n",
    "    - Iterate over the k folds\n",
    "    - Train the model on the k-1 models\n",
    "    - Evaluate the performance on the 1 remaining fold and store it\n",
    "    - Compute the average/median performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7db85a",
   "metadata": {},
   "source": [
    "3. What problem do you see with this approach ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543775f4",
   "metadata": {},
   "source": [
    "## Using sklearn\n",
    "Sklearn is THE usual library for machine learning (but not so much deep learning), which comes with built-in methods (and many more) for training and performance evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e5fc26",
   "metadata": {},
   "source": [
    "1. Import different performance evaluation metrics by reading the documentation [here](https://scikit-learn.org/stable/modules/model_evaluation.html). (it's too long a read for a lab, but it's definitely an interesting read). Compare the `balanced_accuracy` and `accuracy` to our previous implementation (see [here](https://scikit-learn.org/stable/modules/model_evaluation.html#balanced-accuracy-score) for more). Compute the scores on `y` for the random classifier we implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f9d97a",
   "metadata": {},
   "source": [
    "2. Plenty of functions are available to split the dataset into train and test (see [here](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection) for the complete list). Split `X` and `y` into train and test using the function `sklearn.model_selection.train_test_split`. What is the role of the `stratify` variable ? What problem does it solve ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6707536",
   "metadata": {},
   "source": [
    "3. Use the function `sklearn.model_selection.KFold` to get the proper indexes and perform cross validation on the random classifier using `balanced_accuracy`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafb1b5b",
   "metadata": {},
   "source": [
    "# Conclusion and further works\n",
    "What do you think could be the use of this random classifier for the rest of our work on the titanic dataset ?\n",
    "\n",
    "\n",
    "**Highly advised bonus** (you will be able to use it during the exam): \n",
    "Create a Python module `utils.py` with the different functions and tools we coded today. We will re-use it throughout the rest of the labs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
